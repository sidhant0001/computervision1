{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4445c57e-6b44-46d0-b5de-b7e18727bce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir=None, transform=None, create_dummy_masks=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.create_dummy_masks = create_dummy_masks\n",
    "        \n",
    "        # Get all image files from class subdirectories (your original structure)\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = {}\n",
    "        \n",
    "        for label, class_dir in enumerate(sorted(os.listdir(image_dir))):\n",
    "            if os.path.isdir(os.path.join(image_dir, class_dir)):\n",
    "                self.class_names[label] = class_dir\n",
    "                class_path = os.path.join(image_dir, class_dir)\n",
    "                \n",
    "                for img_name in sorted(os.listdir(class_path)):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.image_paths.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        class_label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path}: {e}\")\n",
    "            image = Image.new('RGB', (256, 256), (0, 0, 0))\n",
    "        \n",
    "        # Create or load mask\n",
    "        if self.create_dummy_masks:\n",
    "            # Create a dummy segmentation mask for demonstration\n",
    "            # This creates simple geometric patterns based on class\n",
    "            mask = self._create_dummy_mask(image.size, class_label)\n",
    "        else:\n",
    "            # Try to load real mask (if mask_dir is provided)\n",
    "            img_name = os.path.basename(image_path)\n",
    "            mask_name = os.path.splitext(img_name)[0] + '.png'\n",
    "            mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "            try:\n",
    "                mask = Image.open(mask_path).convert('L')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {mask_path}: {e}\")\n",
    "                mask = Image.new('L', image.size, 0)\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply same transform to both image and mask\n",
    "            seed = np.random.randint(2147483647)\n",
    "            \n",
    "            # Transform image\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.transform['image'](image)\n",
    "            \n",
    "            # Transform mask\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.transform['mask'](mask)\n",
    "            mask = mask.squeeze(0).long()\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def _create_dummy_mask(self, image_size, class_label):\n",
    "        \"\"\"Create a dummy segmentation mask for demonstration purposes\"\"\"\n",
    "        width, height = image_size\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        # Create different patterns based on class\n",
    "        if class_label == 0:\n",
    "            # Horizontal stripes\n",
    "            mask[height//4:3*height//4, :] = 1\n",
    "        elif class_label == 1:\n",
    "            # Vertical stripes  \n",
    "            mask[:, width//4:3*width//4] = 2\n",
    "        elif class_label == 2:\n",
    "            # Circle in center\n",
    "            center_x, center_y = width//2, height//2\n",
    "            radius = min(width, height) // 4\n",
    "            y, x = np.ogrid[:height, :width]\n",
    "            mask_circle = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "            mask[mask_circle] = 3\n",
    "        else:\n",
    "            # Random pattern for other classes\n",
    "            mask[::2, ::2] = (class_label % 5) + 1\n",
    "            \n",
    "        return Image.fromarray(mask, mode='L')\n",
    "\n",
    "# Custom CNN for Semantic Segmentation (UNet-like architecture)\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Handle size mismatch\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                       diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class CustomSegmentationCNN(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=21):  # 21 for PASCAL VOC, adjust as needed\n",
    "        super(CustomSegmentationCNN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Encoder (Downsampling path)\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        \n",
    "        # Decoder (Upsampling path)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# Data transforms for segmentation\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),  # Use nearest for masks\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_transform = {\n",
    "    'image': image_transform,\n",
    "    'mask': mask_transform\n",
    "}\n",
    "\n",
    "test_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = {\n",
    "    'image': test_image_transform,\n",
    "    'mask': test_mask_transform\n",
    "}\n",
    "\n",
    "# Reusing original paths from your classification code\n",
    "train_image_dir = '/Workspace/sid-v2/computervision1/Classification_dataset_v3/images/train'\n",
    "test_image_dir = '/Workspace/sid-v2/computervision1/Classification_dataset_v3/images/test'\n",
    "\n",
    "# Create datasets with dummy masks for demonstration\n",
    "print(\"Creating datasets with dummy masks for demonstration...\")\n",
    "training_dataset = SegmentationDataset(image_dir=train_image_dir, transform=train_transform, create_dummy_masks=True)\n",
    "test_dataset = SegmentationDataset(image_dir=test_image_dir, transform=test_transform, create_dummy_masks=True)\n",
    "\n",
    "# Number of classes from your original classification dataset\n",
    "num_classes = len(training_dataset.class_names) + 1  # +1 for background class\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Class names:\", training_dataset.class_names)\n",
    "print(\"Note: Using dummy masks for demonstration. Class 0 = background, Classes 1+ = your original classes\")\n",
    "\n",
    "train_loader = DataLoader(dataset=training_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CustomSegmentationCNN(n_channels=3, n_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)  # 255 is typically used for ignore class\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# Metrics\n",
    "def calculate_iou(pred, target, num_classes):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) for each class\"\"\"\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        \n",
    "        if target_inds.sum().item() == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            intersection = (pred_inds & target_inds).sum().item()\n",
    "            union = (pred_inds | target_inds).sum().item()\n",
    "            if union == 0:\n",
    "                ious.append(float('nan'))\n",
    "            else:\n",
    "                ious.append(intersection / union)\n",
    "    \n",
    "    return ious\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pixel_correct = 0\n",
    "    pixel_total = 0\n",
    "    \n",
    "    for images, masks in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate pixel accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        mask_pixels = (masks != 255)  # Ignore index\n",
    "        pixel_total += mask_pixels.sum().item()\n",
    "        pixel_correct += ((predicted == masks) & mask_pixels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    pixel_acc = 100 * pixel_correct / pixel_total if pixel_total > 0 else 0\n",
    "    \n",
    "    return train_loss, pixel_acc\n",
    "\n",
    "def validate_model(model, test_loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    pixel_correct = 0\n",
    "    pixel_total = 0\n",
    "    all_ious = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc=\"Validating\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate pixel accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            mask_pixels = (masks != 255)\n",
    "            pixel_total += mask_pixels.sum().item()\n",
    "            pixel_correct += ((predicted == masks) & mask_pixels).sum().item()\n",
    "            \n",
    "            # Calculate IoU for this batch\n",
    "            for i in range(predicted.size(0)):\n",
    "                pred_mask = predicted[i][mask_pixels[i]]\n",
    "                true_mask = masks[i][mask_pixels[i]]\n",
    "                if len(pred_mask) > 0:\n",
    "                    ious = calculate_iou(pred_mask, true_mask, num_classes)\n",
    "                    all_ious.append(ious)\n",
    "    \n",
    "    val_loss = running_loss / len(test_loader)\n",
    "    pixel_acc = 100 * pixel_correct / pixel_total if pixel_total > 0 else 0\n",
    "    \n",
    "    # Calculate mean IoU\n",
    "    if all_ious:\n",
    "        mean_ious = []\n",
    "        for cls in range(num_classes):\n",
    "            class_ious = [iou[cls] for iou in all_ious if not np.isnan(iou[cls])]\n",
    "            if class_ious:\n",
    "                mean_ious.append(np.mean(class_ious))\n",
    "        mean_iou = np.mean(mean_ious) if mean_ious else 0\n",
    "    else:\n",
    "        mean_iou = 0\n",
    "    \n",
    "    return val_loss, pixel_acc, mean_iou\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_ious = []\n",
    "\n",
    "print(\"Starting Custom CNN Segmentation training...\")\n",
    "print(f\"Model: Custom UNet-like CNN\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, mean_iou = validate_model(model, test_loader, criterion, device, num_classes)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_ious.append(mean_iou)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Pixel Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Pixel Acc: {val_acc:.2f}%, Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Pixel Accuracy', linewidth=2)\n",
    "plt.plot(val_accuracies, label='Validation Pixel Accuracy', linewidth=2)\n",
    "plt.title('Training and Validation Pixel Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(val_ious, label='Validation Mean IoU', linewidth=2, color='green')\n",
    "plt.title('Validation Mean IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'custom_segmentation_cnn.pth')\n",
    "print(\"Model saved as 'custom_segmentation_cnn.pth'\")\n",
    "\n",
    "def predict_segmentation(model, image_path, transform, device, num_classes):\n",
    "    \"\"\"Make segmentation prediction on a single image\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_size = image.size\n",
    "    \n",
    "    image_tensor = transform['image'](image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predicted_mask = torch.argmax(probabilities, dim=1).squeeze(0)\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    predicted_mask = predicted_mask.cpu().numpy().astype(np.uint8)\n",
    "    predicted_mask_pil = Image.fromarray(predicted_mask, mode='L')\n",
    "    predicted_mask_pil = predicted_mask_pil.resize(original_size, Image.NEAREST)\n",
    "    \n",
    "    return predicted_mask_pil, probabilities.cpu()\n",
    "\n",
    "def visualize_segmentation(image_path, predicted_mask, num_classes):\n",
    "    \"\"\"Visualize original image and predicted segmentation\"\"\"\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Create a colormap for visualization\n",
    "    predicted_mask_array = np.array(predicted_mask)\n",
    "    colored_mask = plt.cm.tab20(predicted_mask_array / num_classes)\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.title('Predicted Segmentation')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nModel ready for inference!\")\n",
    "print(\"Note: This demo uses dummy masks created from your classification dataset\")\n",
    "print(\"For real segmentation, you would need:\")\n",
    "print(\"1. Actual pixel-level annotated masks\")\n",
    "print(\"2. Images and masks in the proper segmentation dataset structure\")\n",
    "print(\"\\nUse predict_segmentation() for single image prediction\")\n",
    "print(\"Use visualize_segmentation() to visualize results\")\n",
    "\n",
    "# Example of how to use:\n",
    "# predicted_mask, probabilities = predict_segmentation(model, 'path/to/image.jpg', test_transform, device, num_classes)\n",
    "# visualize_segmentation('path/to/image.jpg', predicted_mask, num_classes)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully:\")\n",
    "print(f\"Training samples: {len(training_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {training_dataset.class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cef9db2-7684-403d-839e-86b3e54f7375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6-Semantic Segmentation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
