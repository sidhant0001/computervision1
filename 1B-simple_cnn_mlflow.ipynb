{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80524c3e-41a8-4c29-94cb-6cb2153429fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70cbf678-f2d0-4ec0-934f-f03bd9f07171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from  PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# MLflow imports - these are essential for tracking\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import mlflow.sklearn  # For potential metric logging\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74b22aa-25f6-4d86-b909-7f9914741e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_dir ='/Workspace/sid-v2/computervision1/Classification_dataset_v3/images/train'\n",
    "for label, class_dir in enumerate(os.listdir(image_dir)):\n",
    "    print(label,class_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a4379d-9b6d-4201-992c-7852995bae71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class generate_image_dataset(Dataset):\n",
    "  def __init__(self,image_dir, transform=None):\n",
    "    self.image_dir=image_dir\n",
    "    self.image_paths=[]\n",
    "    self.labels=[]\n",
    "    self.class_name={}\n",
    "    self.transform = transform\n",
    "\n",
    "    for label, class_dir in enumerate(os.listdir(image_dir)):\n",
    "        self.class_name[label] =class_dir\n",
    "        class_path = os.path.join(image_dir,class_dir)\n",
    "        for img_name in os.listdir(class_path):\n",
    "          self.image_paths.append(os.path.join(class_path,img_name))\n",
    "          self.labels.append(label)\n",
    "\n",
    "# Log dataset statistics - this helps other data scientists understand your data\n",
    "        self.dataset_stats = {\n",
    "            'total_images': len(self.image_paths),\n",
    "            'num_classes': len(self.class_name),\n",
    "            'class_distribution': {self.class_name[i]: self.labels.count(i) for i in self.class_name.keys()},\n",
    "            'class_names': list(self.class_name.values())\n",
    "        }\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_paths)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    image_path = self.image_paths[idx]\n",
    "    image = Image.open(image_path).convert('RGB')     \n",
    "    label = self.labels[idx]             \n",
    "\n",
    "    if self.transform:\n",
    "        image = self.transform(image)     \n",
    "\n",
    "    return image, label\n",
    "  \n",
    "  def get_stats(self):\n",
    "        \"\"\"Return dataset statistics for MLflow logging\"\"\"\n",
    "        return self.dataset_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351770ea-1e90-4f53-9ec0-3a32c9562b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128,128)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5,0.5,0.5])]\n",
    "#     )\n",
    "\n",
    "# Define transforms with detailed configuration for logging, define the confog separately\n",
    "transform_config = {\n",
    "    'resize': (128, 128),\n",
    "    'normalize_mean': [0.5, 0.5, 0.5],\n",
    "    'normalize_std': [0.5, 0.5, 0.5]\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(transform_config['resize']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=transform_config['normalize_mean'], \n",
    "                        std=transform_config['normalize_std'])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_image_dir ='/Workspace/sid-v2/computervision1/Classification_dataset_v3/images/train'\n",
    "test_image_dir ='/Workspace/sid-v2/computervision1/Classification_dataset_v3/images/test'\n",
    "\n",
    "# Create datasets\n",
    "training_image_dataset = generate_image_dataset(image_dir=train_image_dir,transform=transform)\n",
    "test_image_dataset = generate_image_dataset(image_dir=test_image_dir,transform=transform)\n",
    "\n",
    "# Create data loaders with configuration\n",
    "batch_size = 32\n",
    "train_image_loader = DataLoader(dataset=training_image_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_image_loader = DataLoader(dataset=test_image_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prepare configuration dictionary for MLflow logging\n",
    "data_config = {\n",
    "    'train_data_path': train_image_dir,\n",
    "    'test_data_path': test_image_dir,\n",
    "    'batch_size': batch_size,\n",
    "    'transform_config': transform_config,\n",
    "    'train_dataset_stats': training_image_dataset.get_stats(),\n",
    "    'test_dataset_stats': test_image_dataset.get_stats()\n",
    "}\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(training_image_dataset)}\")\n",
    "print(f\"Test samples: {len(test_image_dataset)}\")\n",
    "print(f\"Classes: {training_image_dataset.class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c719212-bfec-40ad-8502-9f419fc97281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for images, labels in train_image_loader:\n",
    "    print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e7d975-9e4b-4521-91da-d8e593d1ce8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Sample Visualization with Artifact Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22844f25-b913-4a59-9550-974e79fafdfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_sample_visualization(data_loader, dataset, save_path=\"sample_images.png\"):\n",
    "    \"\"\"Create and save sample images for MLflow artifact logging\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    fig.suptitle('Sample Training Images', fontsize=16)\n",
    "    \n",
    "    # Get one batch\n",
    "    images, labels = next(iter(data_loader))\n",
    "    \n",
    "    for i in range(8):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        img = images[i].numpy()\n",
    "        label = labels[i].item()\n",
    "        \n",
    "        # Convert from tensor format to displayable format\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        # Denormalize for display\n",
    "        img = img * 0.5 + 0.5  # Reverse the normalization\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].set_title(f'{dataset.class_name[label]}')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return save_path\n",
    "\n",
    "# Create visualization\n",
    "sample_viz_path = create_sample_visualization(train_image_loader, training_image_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aa80096-2f2f-4dcc-867b-384bd0e32f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### improved model definition for better integration with mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f5ea20-3866-4b2e-85f7-6a2558f8f407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomCNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN model for image classification\n",
    "    Enhanced with proper documentation for MLflow tracking\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CustomCNNModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define architecture configuration for logging\n",
    "        self.architecture_config = {\n",
    "            'input_dim': input_dim,\n",
    "            'num_classes': num_classes,\n",
    "            'conv_layers': [\n",
    "                {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "                {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "                {'out_channels': 128, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "            ],\n",
    "            'fc_layers': [512, 128, num_classes]\n",
    "        }\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),    \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),    \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),    \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self._to_linear = None\n",
    "        self.get_conv_output(self.input_dim)\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def get_conv_output(self, input_dim=128):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, input_dim, input_dim)\n",
    "            output = self.conv_layers(dummy_input)\n",
    "            self._to_linear = output.view(output.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "    def get_architecture_config(self):\n",
    "        \"\"\"Return model architecture configuration for logging\"\"\"\n",
    "        return self.architecture_config\n",
    "\n",
    "# Model initialization remains the same\n",
    "device = torch.device(\"cpu\")\n",
    "model = CustomCNNModel(input_dim=128, num_classes=3).to(device)\n",
    "\n",
    "print(\"Model initialized successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "410581ab-22df-4183-a7a0-a0733d17ee50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## training with mlflow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f582785-6c7e-401e-9898-a151efc7986c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Get the full path of the current notebook (e.g., \"/Users/your.username/Folder/MyNotebook\")\n",
    "notebook_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "\n",
    "# Extract the folder path (e.g., \"/Users/your.username/Folder\")\n",
    "folder_path = os.path.dirname(notebook_path)\n",
    "\n",
    "# the folder for tracking the experiments cannot exist within the current folder as it is part of a it repo \n",
    "experiment_path = f\"/Workspace/sid-v2/experiment_tracker\"\n",
    "experiment_name = experiment_path + \"/3class_cnn_classifier\"\n",
    "\n",
    "# Create the experiment if it does not exist\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d15c8f8-491c-4919-b438-237c6b114d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model_with_mlflow():\n",
    "    \"\"\"\n",
    "    Train the model with comprehensive MLflow tracking\n",
    "    This is the core function that implements all MLflow best practices\n",
    "    \"\"\"  \n",
    "\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"cnn_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "        \n",
    "        # Enable autologging for PyTorch (captures model summary, parameters, etc.)\n",
    "        mlflow.pytorch.autolog(log_models=False)  # We'll manually log the model for better control\n",
    "        \n",
    "        # Log comprehensive parameters\n",
    "        training_params = {\n",
    "            'model_type': 'CustomCNN',\n",
    "            'optimizer': 'Adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': 2,\n",
    "            'device': str(device),\n",
    "            'input_size': 128,\n",
    "            'num_classes': 3\n",
    "        }\n",
    "        \n",
    "        # Log all parameters\n",
    "        mlflow.log_params(training_params)\n",
    "        mlflow.log_params(transform_config)\n",
    "        mlflow.log_params(model.get_architecture_config())\n",
    "        \n",
    "        # Log dataset information\n",
    "        mlflow.log_params(data_config['train_dataset_stats'])\n",
    "        mlflow.log_dict(data_config, \"data_configuration.json\")\n",
    "        \n",
    "        # Set up training\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=training_params['learning_rate'])\n",
    "        epochs = training_params['epochs']\n",
    "        \n",
    "        # Log sample images as artifact\n",
    "        mlflow.log_artifact(sample_viz_path, \"visualizations\")\n",
    "        \n",
    "        # Training loop with detailed logging\n",
    "        training_history = {\n",
    "            'epoch_losses': [],\n",
    "            'batch_losses': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            batch_losses = []\n",
    "            \n",
    "            # Log learning rate for this epoch\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            mlflow.log_metric(\"learning_rate\", current_lr, step=epoch)\n",
    "            training_history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(train_image_loader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_loss = loss.item()\n",
    "                epoch_loss += batch_loss\n",
    "                batch_losses.append(batch_loss)\n",
    "                \n",
    "                # Log batch loss every 10 batches\n",
    "                if batch_idx % 10 == 0:\n",
    "                    step = epoch * len(train_image_loader) + batch_idx\n",
    "                    mlflow.log_metric(\"batch_loss\", batch_loss, step=step)\n",
    "            \n",
    "            # Calculate and log epoch metrics\n",
    "            avg_epoch_loss = epoch_loss / len(train_image_loader)\n",
    "            training_history['epoch_losses'].append(avg_epoch_loss)\n",
    "            training_history['batch_losses'].extend(batch_losses)\n",
    "            \n",
    "            # Log epoch metrics\n",
    "            mlflow.log_metric(\"epoch_loss\", avg_epoch_loss, step=epoch)\n",
    "            mlflow.log_metric(\"epoch\", epoch + 1, step=epoch)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate model on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        class_correct = {i: 0 for i in range(3)}\n",
    "        class_total = {i: 0 for i in range(3)}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_image_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Class-wise accuracy\n",
    "                for i in range(labels.size(0)):\n",
    "                    label = labels[i].item()\n",
    "                    class_correct[label] += (predicted[i] == labels[i]).item()\n",
    "                    class_total[label] += 1\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        test_accuracy = 100 * correct / total\n",
    "        avg_test_loss = test_loss / len(test_image_loader)\n",
    "        \n",
    "        # Log final metrics\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_loss\", avg_test_loss)\n",
    "        mlflow.log_metric(\"final_train_loss\", training_history['epoch_losses'][-1])\n",
    "        \n",
    "        # Log class-wise accuracy\n",
    "        for class_idx in range(3):\n",
    "            if class_total[class_idx] > 0:\n",
    "                class_acc = 100 * class_correct[class_idx] / class_total[class_idx]\n",
    "                class_name = training_image_dataset.class_name[class_idx]\n",
    "                mlflow.log_metric(f\"accuracy_{class_name}\", class_acc)\n",
    "        \n",
    "        # Create and log training history plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(training_history['epoch_losses'])\n",
    "        plt.title('Training Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(training_history['batch_losses'])\n",
    "        plt.title('Training Loss per Batch')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        training_plot_path = \"training_history.png\"\n",
    "        plt.savefig(training_plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Log training history as artifact\n",
    "        mlflow.log_artifact(training_plot_path, \"plots\")\n",
    "        \n",
    "        # Save and log training history as JSON\n",
    "        history_path = \"training_history.json\"\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(training_history, f, indent=2)\n",
    "        mlflow.log_artifact(history_path, \"metrics\")\n",
    "        \n",
    "        # Log model with signature for deployment\n",
    "        # Create sample input for signature inference\n",
    "        sample_input = torch.randn(1, 3, 128, 128)\n",
    "        with torch.no_grad():\n",
    "            sample_output = model(sample_input)\n",
    "        \n",
    "        # Infer signature\n",
    "        signature = infer_signature(\n",
    "            sample_input.numpy(), \n",
    "            sample_output.numpy()\n",
    "        )\n",
    "        \n",
    "        # Log the model\n",
    "        model_info = mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            input_example=sample_input.numpy(),\n",
    "            registered_model_name=\"image_classification_cnn\"  # This registers to Unity Catalog\n",
    "        )\n",
    "        \n",
    "        # Log additional model artifacts\n",
    "        torch.save(model.state_dict(), \"model_state_dict.pth\")\n",
    "        mlflow.log_artifact(\"model_state_dict.pth\", \"model_files\")\n",
    "        \n",
    "        # Log run summary\n",
    "        run_summary = {\n",
    "            'run_id': run.info.run_id,\n",
    "            'model_uri': model_info.model_uri,\n",
    "            'final_test_accuracy': test_accuracy,\n",
    "            'final_train_loss': training_history['epoch_losses'][-1],\n",
    "            'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'training_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        mlflow.log_dict(run_summary, \"run_summary.json\")\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        print(f\"Model registered as: image_classification_cnn\")\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        \n",
    "        return model, run_summary\n",
    "\n",
    "# Execute training\n",
    "trained_model, run_info = train_model_with_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b575c2-ab75-46e4-bc50-f765e02fed48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1B-simple_cnn_mlflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
